Stochastic gradient descent (SGD) is one of the most important optimizer to reduce loss.

Formula used is: W(new) = W(old) - eta*(del(L)/del(W))

W = weights

eta = Learning_rate

L = loss_function (SGD generally work good for convex loss function)

convex_loss_function:- https://www.solver.com/convex-optimization
